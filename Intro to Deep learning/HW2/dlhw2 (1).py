# -*- coding: utf-8 -*-
"""DLhw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W5b-CEhRPMShv4OiIwwC_BckMXcU8kFn
"""

import torch
import math
"""
x1 = torch.tensor([1.],requires_grad=True)
x2 = torch.tensor([3.],requires_grad=True)
w1 = torch.tensor([2.],requires_grad=True)
w2 = torch.tensor([-1.],requires_grad=True)

f1 = x1*w1
f2 = x2*w2
f3 = torch.sin(f1)
f4 = torch.cos(f2)
f5 = f3**2
f6 = f4+f5+2
f = 1/f6
df = -1/f6^2
df6 = f4+f5
df5 = 2*f3
df4 = -torch.sin(f2)
df3 = torch.cos(f1)
df2x2 = w2
df2w2 = w1

f.backward()
print(x1.grad, x2.grad, w1.grad, w2.grad)
"""
W=torch.tensor([[0.1,0.5,0.4],[-0.3,0.8,0.6],[0.7,1.0,-0.2]])
X = torch.tensor([[0.2],[0.5],[0.4]])

F1=torch.mm(W,X)
print("F1 = ", F1)
F2 = torch.sigmoid(F1)
print("F2 = ", F2)
F3 = torch.norm(F2)**2
print("F3 = ", F3)
"""
F1.retain_grad()
F2.retain_grad()
F3.retain_grad()

F3.backward()
""
W=torch.tensor([[0.1,0.5],[-0.3,0.8]],requires_grad = True)
X = torch.tensor([[0.2],[0.4]],requires_grad=True)
WX=torch.mm(W,X)
norm_WX = torch.norm(WX)**2
WX.retain_grad()

print(norm_WX)
norm_WX.backward()
"""
#norm_WX.grad

#print(F2.grad)
dF3 = 2 * F2
print("dF3 = ", dF3)
#print(F1.grad)
dF2 = F2 * (1-F2) * dF3
print("dF2 = ", dF2)
#print(W.grad)
dW = torch.mm(dF2,torch.t(X))
print("dW = ", dW)
#print(X.grad)
dX = torch.mm(torch.t(W),dF2)
print("dX = ", dX)
#WX.grad